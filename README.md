# Dynamic Batching Inference System

A high-performance inference server implementation using **FastAPI** and **PyTorch**, featuring a dynamic batching mechanism to optimize throughput and latency.
